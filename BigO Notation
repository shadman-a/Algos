BigO Notation 

array[0] are costly

LinkedList is not costly 


Constant time: 
O(1)

Constant time is not a big deal 

i.e a print statement is O(1)

Linear time:
O(n) 

Linear time a loop, if theres two loops O(n + n) which is the same as O(n) still linear because approx

loop is O(n)

Quadratic time:
O(n^2)

Nested loop is O(n^2)
Quadratic time gets slower with larger times
The more loops that are nested the slower the code gets 

Logarithmic time: THE BEST KIND
O(log^n)
Logarithmic curve stops getting slower at some point (asymptote)

Binary search

Exponentional time: 
O(2^n) the worst kind 
opposite of logarithmic 


Arrays:
have fixed size in java 

Lookup: by index is the best O(1)
Insert: O(n)
Delete: O(n)
Making an array 

import java.util.Arrays;
public class Main {
public static void main (String [] args) {
int [] numbers = new int [3];
System.out.println(Arrays.toString(numbers))
}

Two kinds of dynamic arrays in Java
Vector-grows by 100% synchronized only a single thread **not good?**
ArrayList- grows by 50%


Linked Lists:
Each node points to the next one
Head ----  Tail

Lookup by value O(n)
By Index O(n) not like array they can be anywhere
Insert at the end O(1)
Insert at the beginning O(1)
In the middle O(n)

Deleting
From the beginning O(1)
From the end O(n)
From the middle O(n)

Type of linked list:
- Singly: contains reference only for next node
- doubly: contains a references for both prev and next

circular: the tail links to head

Stack:
-peek
-push
-pop
-empty?
Stacks are used to not store data and retrieve

**Doing something in reverse is where to use stack.**

Queues - not a class but a interface
-First in first out
-It's like a line of people

Uses:
a resource has to be shared by a lot of people
- printers
- operating systems
- Web servers

Operations:
enqueue: add to end of queue
dequeue: remove from beginning of queue
peek: see first item
isEmpty:
isFull:
all O(1) Linear

Implements the queue interface:
ArrayDeque
-double ended queue
add and offer
poll and remove

peek and element



Hash Tables:
Super fast lookups
- spell checkers
- dictionaries
- compilers

Hashmap -deterministic
- key value pairs

operations:

insert
lookup
delete

containsKey is O(1)
but
containsValue is O(n)



Sets:
only has keys:
useful to remove duplicates


.hashCode can make a has out of anything

Collisions
- two keys generated has the same key

Chaining
- saving multiple values at the same key
Open Addressing
- if a key value pair is taken the table need to be probed to find an empty slot (Linear Probing).
    (hash(key) +i) % table_size   next to each other and form a cluster
    - Quadratic Probing- (hash(key) + i^2) % table_size   spaces it out and prevents clusters   can cause an infinite loop

Double hashing
hash2(key) = prime - (key%prime)
(hash1(key) + i * hash2(key))% table_size


------------------------------------------------
Non-Linear data Structures

Trees:
           1. Root
            /  \
           2    3
          / \  / \ edges
         4   5 6  7 Leaf nodes

Represents Hierarchical data
Databases
Autocompletion
compilers
Compression

Binary search tree

left< node < right

Logarithmic time complexity O(log n)
Lookup O(log n)
Insert O(log n)
Delete O(log n)

Really good performance

Tree traversal
Breadth first-  level order
Depth first-
    Pre-order Root -> left -> right
    In-order left -> root -> right : ascending order
             right -> root -> left : descending order
    post order left -> right -> root

Recursion: repetition w/o a loop
 - a method calls itself
iteration is with a loop